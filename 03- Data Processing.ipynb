{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxRlTMVqu+eC9hRwQ+rH5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianlucaRapaglia/LLM-training/blob/main/03-%20Data%20Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Md0atw-7L4s"
      },
      "outputs": [],
      "source": [
        "pip install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"This course is amazing!\",\n",
        "]\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "batch[\"labels\"] = torch.tensor([1, 1])\n",
        "\n",
        "optimizer = AdamW(model.parameters())\n",
        "loss = model(**batch).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "tJGJveZl7sOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to load datasets we can use the *datasets* library. A powerful feature of this library is the Apache Arrow, that is an open source framework fro columnar, in-memory data. Even if the dataset is several GBs, you can access individual examples instantly without loading everything into RAM, thanks to Arrow memory-mapping."
      ],
      "metadata": {
        "id": "1b3l68Ae-4Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "F_Ajbzlr_iey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can also access each pair of sentences in our raw_dataset object by indexing, like with a dictionary:"
      ],
      "metadata": {
        "id": "pQ5QaGZ1-xEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]"
      ],
      "metadata": {
        "id": "y70Nxpq0Aaxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in order to understand what 'label':1 is referring to, we can print out the information"
      ],
      "metadata": {
        "id": "D59EiDA-BI4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset.features"
      ],
      "metadata": {
        "id": "mkPnACM5Ahus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and we can see that 1:equivalent, 0:not_equivalent. Now we can tokenize the dataset using the tokenizer. Though, this will work only if we have enough RAM to store the dataset during the tokenization."
      ],
      "metadata": {
        "id": "8r2WoZbJBU8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_1 = raw_datasets[\"train\"][\"sentence1\"]\n",
        "tokenized_sentences_1 = tokenizer(list(raw_datasets[\"train\"][\"sentence1\"]))"
      ],
      "metadata": {
        "id": "siLh4DEzg3DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    list(raw_datasets[\"train\"][\"sentence1\"]),\n",
        "    list(raw_datasets[\"train\"][\"sentence2\"]),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ],
      "metadata": {
        "id": "pewAbiDuBRw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more efficient tokenization and to keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so letâ€™s define a function that tokenizes our inputs:"
      ],
      "metadata": {
        "id": "MXNjzaGeiGKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ],
      "metadata": {
        "id": "LYzuZmHwgUkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we apply the tokenization function on all our datasets at once. Weâ€™re using batched=True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing."
      ],
      "metadata": {
        "id": "EI5xNQ7clKTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "mGgKzD-UiStG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform Dynamic Padding in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ðŸ¤— Transformers library provides us with such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:"
      ],
      "metadata": {
        "id": "PKtmozIPoIn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "CFBXab8NlL9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test this new toy, letâ€™s grab a few samples from our training set that we would like to batch together. Here, we remove the columns idx, sentence1, and sentence2 as they wonâ€™t be needed and contain strings (and we canâ€™t create tensors with strings) and have a look at the lengths of each entry in the batch:"
      ],
      "metadata": {
        "id": "H4Tomf8FpSPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "C23lHcEEpSqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Letâ€™s double-check that our data_collator is dynamically padding the batch properly:"
      ],
      "metadata": {
        "id": "WwHRSuP1qY9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "-w2l81wipUga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways:\n",
        "\n",
        "\n",
        "\n",
        "*   Use batched=True with Dataset.map() for significantly faster preprocessing\n",
        "*   Dynamic padding with DataCollatorWithPadding is more efficient than fixed-length padding\n",
        "*   Always preprocess your data to match what your model expects (numerical tensors, correct column names)\n",
        "*   The ðŸ¤— Datasets library provides powerful tools for efficient data processing at scale\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w6-yBX5_tA3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_notebook(path_in, path_out):\n",
        "    import nbformat\n",
        "    nb = nbformat.read(path_in, as_version=nbformat.NO_CONVERT)\n",
        "    nb[\"metadata\"].pop(\"widgets\", None)\n",
        "    nbformat.write(nb, path_out)\n",
        "\n",
        "# Example usage:\n",
        "clean_notebook(\"/content/03_Data_Processing.ipynb\", \"/content/03_Data_Processing_clean.ipynb\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5rWPjihyqvhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YANa9b5_uMSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}