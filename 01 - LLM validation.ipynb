{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMr+ub+CPwqtjJdw8DYNxYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianlucaRapaglia/LLM-training/blob/main/01%20-%20LLM%20validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– Evaluate Fine-Tuned GPT-2 Model on WikiText-2\n",
        "\n",
        "This notebook demonstrates how to validate a **fine-tuned DistilGPT-2 model** on the dataset it was trained on: `wikitext/wikitext-2-raw-v1`.\n",
        "\n",
        "We will:\n",
        "- Load the model and tokenizer from Hugging Face Hub\n",
        "- Load and preprocess the test set of WikiText-2\n",
        "- Evaluate the model using **perplexity**, a standard metric in language modeling\n"
      ],
      "metadata": {
        "id": "eHsP7VI-UKjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Run this to log in to your Hugging Face account\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "cuD3Ae9EU_Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO-R12sKTmSz"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets evaluate fsspec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Your fine-tuned model on WikiText-2\n",
        "model_name = \"lucarapaglia/distilgpt2-finetuned-wikitext2\"\n",
        "\n",
        "# Use original tokenizer from distilgpt2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# Load your fine-tuned model weights\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1l1_K-BHUs1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's load the test dataset:"
      ],
      "metadata": {
        "id": "HkGnm7XmV_I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the test split of the original training dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "\n",
        "# Let's preview a few examples\n",
        "dataset[:3]\n"
      ],
      "metadata": {
        "id": "bt9DSKk-WA9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into input IDs using the model's tokenizer\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], return_special_tokens_mask=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n"
      ],
      "metadata": {
        "id": "QqD_AcS4YLs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Language models need contiguous sequences (not single sentences)\n",
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples}\n",
        "    total_length = len(concatenated[\"input_ids\"])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    return {\n",
        "        k: [concatenated[k][i:i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k in concatenated\n",
        "    }\n",
        "\n",
        "lm_dataset = tokenized_dataset.map(group_texts, batched=True)\n"
      ],
      "metadata": {
        "id": "sYmyVbUtYN_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#from evaluate import load\n",
        "#\n",
        "## Load the evaluation metric\n",
        "#perplexity = load(\"perplexity\")\n",
        "#\n",
        "## Compute perplexity using the raw test data (not tokenized)\n",
        "#texts = [t for t in dataset[\"text\"] if t.strip() != \"\"]\n",
        "#results = perplexity.compute(\n",
        "#    model_id=model_name,\n",
        "#    add_start_token=True,\n",
        "#    predictions=texts,\n",
        "#)\n",
        "#\n",
        "#print(f\"ðŸ“Š Perplexity of the fine-tuned model: {results['perplexity']:.2f}\")\n",
        "\n",
        "# Load your fine-tuned model and move to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval().to(device)\n",
        "\n",
        "def compute_perplexity_on_dataset(dataset, batch_size=16):\n",
        "    losses = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i : i + batch_size]\n",
        "\n",
        "        # Convert batch to tensors and move to device\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch.get(\"attention_mask\", [[1]*input_ids.size(1)]*len(batch[\"input_ids\"]))).to(device)\n",
        "\n",
        "        # Labels are the same as input_ids for causal LM\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            # outputs.loss is the average cross-entropy loss over all non-masked tokens in the batch\n",
        "            loss = outputs.loss\n",
        "            losses.append(loss.item() * input_ids.size(0))  # multiply by batch size to sum losses\n",
        "\n",
        "    # Average loss over all tokens\n",
        "    avg_loss = sum(losses) / len(dataset)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    return perplexity.item()\n",
        "\n",
        "ppl = compute_perplexity_on_dataset(lm_dataset)\n",
        "print(f\"Perplexity on tokenized grouped dataset: {ppl:.2f}\")\n"
      ],
      "metadata": {
        "id": "vrizay25YRIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfIUrsp3kKt5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}